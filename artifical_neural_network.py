# -*- coding: utf-8 -*-
"""Artifical Neural Network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JToPMAHSff6n1FoIbaMnpa2CZRo7a2ma

# **ARTIFICIAL NEURAL NETWORK**

Import the Libraries
"""

import numpy as np
import pandas as pd
import tensorflow as tf

tf.__version__

"""**Data Preprocessing**

Importing the Dataset
"""

db = pd.read_excel("Folds5x2_pp.xlsx")
x = db.iloc[:, :-1].values
y = db.iloc[:, -1].values

print(x)
print(y)

"""Splitting the dataset into the training set and test set

"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state =0)

"""**Building the ANN**

Initializing the ANN
"""

ann = tf.keras.models.Sequential()

"""Adding the **Input Layer and the First Hidden Layer**"""

ann.add(tf.keras.layers.Dense(units=6, activation='relu'))
#Dense - Connection between the Input Layer and the First Hidden Layer
#Need to specify the number of Hidden layers
#Activation= 'relu' rectify (to break the linearity between input and first hidden layer)

"""Adding the **Second Hidden Layer**"""

ann.add(tf.keras.layers.Dense(units=6, activation='relu'))

"""Adding the **Output Layer**"""

ann.add(tf.keras.layers.Dense(units=1))
#Output layer it is recommended to have sigmoid or none activation function. Sigmoid is used for classification (yes no), Soft max if more than 2 classification

"""**Training the Artificial Neural Network**

**Compiling the ANN **- Connect to an optimizer which will reduce the loss during backward propagation. Choose a loss function
"""

#Optimizer - tool to perform stochastic gradient descent. SGT (technique that consists of updating the weights of each of the neurons in the hidden )

ann.compile(optimizer='adam', loss= 'mean_squared_error')

"""**Training the ANN Model on the Training Set** """

#Use Fit Method

ann.fit(x_train, y_train, batch_size = 32, epochs = 100)

"""**Predict the results of the Test Set**"""

y_prediction = ann.predict(x_test)
np.set_printoptions(precision=2)
#Vertical Concatenation then its 1, Horizontal its 0
print(np.concatenate((y_prediction.reshape(len(y_prediction),1) , y_test.reshape(len(y_test),1)), 1))